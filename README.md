ğŸ§  Machine Learning Prediction Workflow â€” Efficient CSV Inference

A lightweight end-to-end ML prediction system that performs CSV-based predictions efficiently on low-end hardware (no GPU, i5 CPU). This project contains both frontend and backend in a single repository, enabling full-stack operation and easy deployment.

ğŸš€ Overview

This project demonstrates:

Loading and preprocessing datasets (prediction.csv)

Running predictions using a pre-trained model (model.joblib or .h5)

Serving inference results via an API

Visualizing predictions through a simple React frontend

Optimizing performance for CPU-only environments


Itâ€™s ideal for hackathons, academic demos, or lightweight production use.

ğŸ—ï¸ Project Structure

project-root/ â”œâ”€â”€ frontend/                 â†’ React + Vite user interface â”‚   â”œâ”€â”€ src/ â”‚   â”œâ”€â”€ public/ â”‚   â””â”€â”€ package.json â”œâ”€â”€ backend/                  â†’ Node.js + Express API server â”‚   â”œâ”€â”€ app.js                â†’ Main backend file â”‚   â”œâ”€â”€ model/                â†’ Folder for trained ML model â”‚   â”œâ”€â”€ routes/               â†’ API routes â”‚   â””â”€â”€ package.json â”œâ”€â”€ ml/                       â†’ Optional Python helper scripts â”‚   â”œâ”€â”€ predict.py â”‚   â”œâ”€â”€ model.joblib â”‚   â””â”€â”€ prediction.csv â”œâ”€â”€ README.md â””â”€â”€ .gitignore

âš™ï¸ Installation

1ï¸âƒ£ Clone the Repository
git clone https://github.com/<your-username>/<your-repo-name>.git
cd <your-repo-name>

2ï¸âƒ£ Install Dependencies
Backend â†’
cd backend
npm install

Frontend â†’
cd ../frontend
npm install

ğŸ§© Running the Application

Start Backend (Node/Express) â†’
cd backend
npm start
Runs at â†’ http://localhost:5000

Start Frontend (React/Vite) â†’
cd ../frontend
npm run dev
Open â†’ http://localhost:5173

ğŸ§  Machine Learning Prediction Flow

1. Load Dataset import pandas as pd
df = pd.read_csv('prediction.csv', chunksize=5000)


2. Preprocessing



Normalize numeric data

One-hot encode categoricals

Handle missing values with median imputation

Convert timestamps and ensure proper datatypes


3. Model Loading from joblib import load
model = load('model.joblib')


4. Prediction for chunk in df:
â€ƒpreds = model.predict(chunk)
â€ƒprint(preds[:5])


5. Optimization for Low Hardware



Use chunked reading to limit memory usage

Limit CPU threads:
â€ƒset OMP_NUM_THREADS=1
â€ƒset MKL_NUM_THREADS=1

Avoid retraining locally â€” use pretrained models for inference only


ğŸ–¼ï¸ Presentation

Full presentation link â†’
https://www.canva.com/api/design/eyJhbGciOiJkaXIiLCJlbmMiOiJBMjU2R0NNIiwiZXhwaXJ5IjoxNzY1NTUwOTkxNzQyfQ..v4k_lu85ObL6vsbR.jGyevBS3T5yr2OcS6auV4JgexQ_yfDxK6MLA2JmN51BnlgQ67kjyIkVe83iC73KpoCVOJ9Inh0UAznw.ixra0A8mXLuIIPikQvyhdg/edit

Slides include:

Introduction

Dataset overview

Model loading & inference

Optimization for low-end hardware

Performance summary with contact details


ğŸ§¾ Tech Stack

Frontend â†’ React.js, Vite, Tailwind CSS
Backend â†’ Node.js, Express.js
ML Layer â†’ Python (pandas, numpy, scikit-learn, joblib)
Data â†’ CSV file (prediction.csv)

ğŸ“ˆ Key Features

âœ… Full-stack single-repo setup (frontend + backend)
âœ… Pretrained model inference on CPU
âœ… Chunked CSV loading for large files
âœ… REST API integration for prediction
âœ… Lightweight and resource-efficient
âœ… Presentation-ready documentation

ğŸ§© Example Backend Route

// backend/routes/predict.js import express from "express";
import { spawn } from "child_process";
const router = express.Router();
router.post("/predict", (req, res) => {
â€ƒconst py = spawn("python", ["../ml/predict.py"]);
â€ƒpy.stdout.on("data", (data) => {
â€ƒâ€ƒres.send({ result: data.toString() });
â€ƒ});
â€ƒpy.stderr.on("data", (data) => {
â€ƒâ€ƒconsole.error(Error: ${data});
â€ƒ});
});
export default router;

ğŸ§  Example Python Script (ml/predict.py)

import pandas as pd
from joblib import load
model = load('model.joblib')
data = pd.read_csv('prediction.csv', chunksize=5000)
for chunk in data:
â€ƒpreds = model.predict(chunk)
â€ƒprint(preds[:5])

ğŸ“Š Results Summary

Achieved accurate inference with minimal CPU load

Used chunked reading and limited threads to maintain stability

Eliminated crashes on older laptops (tested on i5 8th gen, 8 GB RAM)


ğŸ§­ Future Enhancements

Integrate REST upload for new CSV files

Add GPU-based inference (optional)

Implement caching for frequent predictions

Add Dockerfile for simplified deployment


ğŸ“ Contact

ğŸ‘¤ Name: Smit Gopani
ğŸ“§ Email: smitgopani113@gmai.com
ğŸ“± Phone: 9327419483
ğŸ’¼ Role: Data Scientist / ML Developer

ğŸªª License

Open-source under the MIT License


---
